{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def diff_Relu(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumming 0 To be the true value of the neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "true = 0\n",
    "w = np.array([-3,-1,2])\n",
    "b = 1\n",
    "x = np.array([1,-2,3])\n",
    "output = Relu(np.sum(np.dot(x,w)) + b)\n",
    "loss = (output-true) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 6 | loss = 36\n",
      "z = 5.82 | loss = 33.872400000000006\n",
      "z = 5.6453999999999995 | loss = 31.870541159999995\n",
      "z = 5.476038000000001 | loss = 29.98699217744401\n",
      "z = 5.31175686 | loss = 28.21476093975706\n",
      "z = 5.1524041542 | loss = 26.54726856821742\n",
      "z = 4.997832029574 | loss = 24.978324995835766\n",
      "z = 4.8478970686867795 | loss = 23.50210598858187\n",
      "z = 4.702460156626176 | loss = 22.113131524656684\n",
      "z = 4.561386351927392 | loss = 20.80624545154948\n",
      "z = 4.424544761369571 | loss = 19.576596345362915\n",
      "z = 4.2918084185284835 | loss = 18.419619501351963\n",
      "z = 4.163054165972628 | loss = 17.331019988822057\n",
      "z = 4.03816254099345 | loss = 16.306756707482677\n",
      "z = 3.9170176647636463 | loss = 15.34302738607045\n",
      "z = 3.799507134820737 | loss = 14.436254467553686\n",
      "z = 3.6855219207761145 | loss = 13.58307182852126\n",
      "z = 3.574956263152831 | loss = 12.780312283455652\n",
      "z = 3.4677075752582462 | loss = 12.024995827503426\n",
      "z = 3.363676348000499 | loss = 11.314318574097975\n",
      "z = 3.262766057560484 | loss = 10.645642346368783\n",
      "z = 3.16488307583367 | loss = 10.016484883698393\n",
      "z = 3.06993658355866 | loss = 9.424510627071816\n",
      "z = 2.9778384860519 | loss = 8.867522049011871\n",
      "z = 2.8885033314703428 | loss = 8.34345149591527\n",
      "z = 2.8018482315262325 | loss = 7.850353512506676\n",
      "z = 2.717792784580446 | loss = 7.386397619917536\n",
      "z = 2.6362590010430327 | loss = 6.949861520580408\n",
      "z = 2.5571712310117416 | loss = 6.539124704714106\n",
      "z = 2.4804560940813896 | loss = 6.152662434665503\n",
      "z = 2.406042411258948 | loss = 5.789040084776773\n",
      "z = 2.3338611389211796 | loss = 5.446907815766465\n",
      "z = 2.263845304753545 | loss = 5.124995563854671\n",
      "z = 2.1959299456109376 | loss = 4.822108326030855\n",
      "z = 2.13005204724261 | loss = 4.537121723962434\n",
      "z = 2.066150485825332 | loss = 4.268977830076255\n",
      "z = 2.004165971250572 | loss = 4.016681240318748\n",
      "z = 1.944040992113055 | loss = 3.7792953790159114\n",
      "z = 1.8857197623496635 | loss = 3.5559390221160716\n",
      "z = 1.8291481694791734 | loss = 3.345783025909011\n",
      "z = 1.7742737243947984 | loss = 3.1480472490777887\n",
      "z = 1.721045512662954 | loss = 2.96199765665729\n",
      "z = 1.6694141472830655 | loss = 2.786943595148845\n",
      "z = 1.6193317228645738 | loss = 2.622235228675549\n",
      "z = 1.5707517711786363 | loss = 2.467261126660823\n",
      "z = 1.523629218043277 | loss = 2.3214459940751673\n",
      "z = 1.4779203415019788 | loss = 2.1842485358253256\n",
      "z = 1.4335827312569185 | loss = 2.0551594473580463\n",
      "z = 1.3905752493192107 | loss = 1.933699524019185\n",
      "z = 1.3488579918396344 | loss = 1.8194178821496512\n",
      "z = 1.3083922520844453 | loss = 1.7118902853146067\n",
      "z = 1.269140484521912 | loss = 1.6107175694525138\n",
      "z = 1.2310662699862545 | loss = 1.5155241610978696\n",
      "z = 1.194134281886667 | loss = 1.4259566831769857\n",
      "z = 1.1583102534300664 | loss = 1.3416826432012248\n",
      "z = 1.123560945827165 | loss = 1.2623891989880334\n",
      "z = 1.08985411745235 | loss = 1.1877819973278405\n",
      "z = 1.0571584939287788 | loss = 1.1175840812857638\n",
      "z = 1.0254437391109161 | loss = 1.0515348620817766\n",
      "z = 0.9946804269375884 | loss = 0.9893891517327431\n",
      "z = 0.964840014129461 | loss = 0.9309162528653384\n",
      "z = 0.9358948137055771 | loss = 0.875899102320997\n",
      "z = 0.9078179692944098 | loss = 0.824133465373826\n",
      "z = 0.8805834302155773 | loss = 0.7754271775702325\n",
      "z = 0.8541659273091098 | loss = 0.7295994313758314\n",
      "z = 0.828540949489836 | loss = 0.686480104981519\n",
      "z = 0.8036847210051412 | loss = 0.6459091307771117\n",
      "z = 0.779574179374987 | loss = 0.6077359011481843\n",
      "z = 0.7561869539937376 | loss = 0.571818709390327\n",
      "z = 0.7335013453739249 | loss = 0.5380242236653578\n",
      "z = 0.7114963050127071 | loss = 0.5062269920467352\n",
      "z = 0.690151415862326 | loss = 0.4763089768167732\n",
      "z = 0.6694468733864556 | loss = 0.4481591162869011\n",
      "z = 0.6493634671848619 | loss = 0.42167291251434524\n",
      "z = 0.629882563169316 | loss = 0.3967520433847474\n",
      "z = 0.6109860862742363 | loss = 0.3733039976207086\n",
      "z = 0.5926565036860091 | loss = 0.35124173136132447\n",
      "z = 0.5748768085754289 | loss = 0.3304833450378703\n",
      "z = 0.5576305043181661 | loss = 0.3109517793461322\n",
      "z = 0.5409015891886207 | loss = 0.29257452918677546\n",
      "z = 0.524674541512962 | loss = 0.2752833745118369\n",
      "z = 0.508934305267573 | loss = 0.25901412707818716\n",
      "z = 0.49366627610954616 | loss = 0.24370639216786666\n",
      "z = 0.47885628782625966 | loss = 0.22930334439074565\n",
      "z = 0.4644905991914724 | loss = 0.21575151673725307\n",
      "z = 0.4505558812157282 | loss = 0.20300060209808138\n",
      "z = 0.4370392047792562 | loss = 0.1910032665140846\n",
      "z = 0.4239280286358783 | loss = 0.17971497346310206\n",
      "z = 0.4112101877768026 | loss = 0.16909381853143327\n",
      "z = 0.39887388214349795 | loss = 0.15910037385622508\n",
      "z = 0.3869076656791933 | loss = 0.14969754176132244\n",
      "z = 0.37530043570881766 | loss = 0.14085041704322837\n",
      "z = 0.364041422637553 | loss = 0.1325261573959735\n",
      "z = 0.35312017995842615 | loss = 0.12469386149387127\n",
      "z = 0.3425265745596734 | loss = 0.11732445427958349\n",
      "z = 0.33225077732288333 | loss = 0.1103905790316602\n",
      "z = 0.3222832540031969 | loss = 0.10386649581088914\n",
      "z = 0.31261475638310077 | loss = 0.09772798590846545\n",
      "z = 0.303236313691608 | loss = 0.09195226194127527\n",
      "z = 0.2941392242808595 | loss = 0.08651788326054576\n",
      "z = 0.285315047552434 | loss = 0.08140467635984766\n",
      "z = 0.27675559612586087 | loss = 0.07659365998698062\n",
      "z = 0.2684529282420851 | loss = 0.07206697468175009\n",
      "z = 0.26039934039482204 | loss = 0.0678078164780584\n",
      "z = 0.25258736018297734 | loss = 0.06380037452420513\n",
      "z = 0.24500973937748782 | loss = 0.06002977238982451\n",
      "z = 0.23765944719616283 | loss = 0.05648201284158571\n",
      "z = 0.23052966378027762 | loss = 0.05314392588264784\n",
      "z = 0.2236137738668692 | loss = 0.050003119862983315\n",
      "z = 0.2169053606508634 | loss = 0.04704793547908113\n",
      "z = 0.21039819983133712 | loss = 0.044267402492267266\n",
      "z = 0.20408625383639667 | loss = 0.04165119900497413\n",
      "z = 0.197963666221305 | loss = 0.03918961314378026\n",
      "z = 0.1920247562346662 | loss = 0.036873507006982977\n",
      "z = 0.18626401354762623 | loss = 0.034694282742870286\n",
      "z = 0.1806760931411977 | loss = 0.03264385063276675\n",
      "z = 0.17525581034696203 | loss = 0.030714599060370322\n",
      "z = 0.16999813603655345 | loss = 0.02889936625590253\n",
      "z = 0.16489819195545652 | loss = 0.027191413710178584\n",
      "z = 0.15995124619679246 | loss = 0.025584401159906914\n",
      "z = 0.15515270881088894 | loss = 0.024072363051356495\n",
      "z = 0.15049812754656233 | loss = 0.022649686395021344\n",
      "z = 0.1459831837201655 | loss = 0.021311089929075593\n",
      "z = 0.14160368820856073 | loss = 0.020051604514267282\n",
      "z = 0.137355577562304 | loss = 0.018866554687474106\n",
      "z = 0.1332349102354351 | loss = 0.01775154130544445\n",
      "z = 0.12923786292837158 | loss = 0.016702425214292563\n",
      "z = 0.12536072704052104 | loss = 0.015715311884128023\n",
      "z = 0.12159990522930542 | loss = 0.014786536951776058\n",
      "z = 0.11795190807242573 | loss = 0.013912652617925968\n",
      "z = 0.11441335083025317 | loss = 0.013090414848206593\n",
      "z = 0.11098095030534594 | loss = 0.012316771330677665\n",
      "z = 0.10765152179618531 | loss = 0.011588850145034562\n",
      "z = 0.10442197614230009 | loss = 0.01090394910146309\n",
      "z = 0.10128931685803055 | loss = 0.010259525709566512\n",
      "z = 0.09825063735229023 | loss = 0.009653187740131247\n",
      "z = 0.09530311823172133 | loss = 0.009082684344689455\n",
      "z = 0.09244402468476909 | loss = 0.008545897699918197\n",
      "z = 0.08967070394422649 | loss = 0.008040835145853117\n",
      "z = 0.08698058282589993 | loss = 0.007565621788733238\n",
      "z = 0.08437116534112299 | loss = 0.007118493541019113\n",
      "z = 0.08184003038088916 | loss = 0.0066977905727448606\n",
      "z = 0.07938482946946301 | loss = 0.0063019511498957235\n",
      "z = 0.07700328458537886 | loss = 0.005929505836936846\n",
      "z = 0.0746931860478176 | loss = 0.005579072041973895\n",
      "z = 0.07245239046638274 | loss = 0.005249348884293189\n",
      "z = 0.07027881875239106 | loss = 0.0049391123652314335\n",
      "z = 0.06817045418981948 | loss = 0.004647210824446277\n",
      "z = 0.06612534056412489 | loss = 0.004372560664721501\n",
      "z = 0.06414158034720152 | loss = 0.004114142329436509\n",
      "z = 0.06221733293678566 | loss = 0.003870996517766834\n",
      "z = 0.06035081294868194 | loss = 0.003642220623566796\n",
      "z = 0.05854028856022153 | loss = 0.003426965384714004\n",
      "z = 0.05678407990341516 | loss = 0.003224431730477438\n",
      "z = 0.055080557506312355 | loss = 0.0030338678152061825\n",
      "z = 0.053428140781123123 | loss = 0.0028545662273275116\n",
      "z = 0.05182529655768919 | loss = 0.002685861363292431\n",
      "z = 0.05027053766095846 | loss = 0.0025271269567218426\n",
      "z = 0.04876242153112986 | loss = 0.0023777737535795973\n",
      "z = 0.04729954888519583 | loss = 0.00223724732474303\n",
      "z = 0.04588056241864025 | loss = 0.002105026007850744\n",
      "z = 0.04450414554608073 | loss = 0.0019806189707867374\n",
      "z = 0.04316902117969834 | loss = 0.001863564389613244\n",
      "z = 0.04187395054430754 | loss = 0.0017534277341871133\n",
      "z = 0.04061773202797825 | loss = 0.00164980015509665\n",
      "z = 0.03939920006713904 | loss = 0.001552296965930449\n",
      "z = 0.038217224065124955 | loss = 0.001460556215243966\n",
      "z = 0.03707070734317086 | loss = 0.001374237342923022\n",
      "z = 0.035958586122875835 | loss = 0.0012930199159562786\n",
      "z = 0.03487982853918947 | loss = 0.0012166024389232565\n",
      "z = 0.03383343368301395 | loss = 0.0011447012347829027\n",
      "z = 0.03281843067252366 | loss = 0.0010770493918072417\n",
      "z = 0.031833877752347584 | loss = 0.0010133957727514104\n",
      "z = 0.030878861419777137 | loss = 0.0009535040825818009\n",
      "z = 0.029952495577183602 | loss = 0.0008971519913012032\n",
      "z = 0.02905392070986801 | loss = 0.0008441303086152972\n",
      "z = 0.02818230308857217 | loss = 0.0007942422073761444\n",
      "z = 0.02733683399591469 | loss = 0.0007473024929201971\n",
      "z = 0.026516728976037407 | loss = 0.0007031369155886218\n",
      "z = 0.025721227106756173 | loss = 0.0006615815238773285\n",
      "z = 0.024949590293553703 | loss = 0.0006224820558161892\n",
      "z = 0.02420110258474739 | loss = 0.0005856933663174669\n",
      "z = 0.023475069507204793 | loss = 0.0005510788883680963\n",
      "z = 0.02277081742198883 | loss = 0.0005185101260655501\n",
      "z = 0.02208769289932877 | loss = 0.00048786617761505856\n",
      "z = 0.021425062112348958 | loss = 0.0004590332865180108\n",
      "z = 0.02078231024897792 | loss = 0.0004319044192847727\n",
      "z = 0.020158840941509104 | loss = 0.0004063788681050637\n",
      "z = 0.01955407571326373 | loss = 0.00038236187700005044\n",
      "z = 0.0189674534418659 | loss = 0.0003597642900693506\n",
      "z = 0.018398429838609953 | loss = 0.0003385022205262531\n",
      "z = 0.01784647694345176 | loss = 0.0003184967392931553\n",
      "z = 0.017311082635148578 | loss = 0.00029967358200094264\n",
      "z = 0.016791750156094354 | loss = 0.00028196287330469475\n",
      "z = 0.01628799765141109 | loss = 0.0002652988674923732\n",
      "z = 0.01579935772186858 | loss = 0.0002496197044235683\n",
      "z = 0.015325376990212192 | loss = 0.0002348671798921253\n",
      "z = 0.014865615680506372 | loss = 0.00022098652956051694\n",
      "z = 0.014419647210091258 | loss = 0.0002079262256634926\n",
      "z = 0.013987057793788504 | loss = 0.00019563778572677975\n"
     ]
    }
   ],
   "source": [
    "for _ in range(200):\n",
    "    z = np.dot(x, w) + b\n",
    "    output = Relu(z)\n",
    "    loss = (output - true) ** 2\n",
    "\n",
    "    dL_dout = 2 * (output - true)\n",
    "    dout_dz = diff_Relu(z)\n",
    "    dz_dw = x\n",
    "    dz_db = 1\n",
    "\n",
    "    # Gradient calculation\n",
    "    gradient = dL_dout * dout_dz * dz_dw\n",
    "    bias_grad = dL_dout * dout_dz * dz_db\n",
    "\n",
    "    # Update weights and bias\n",
    "    w = w - learning_rate * gradient\n",
    "    b = b - learning_rate * bias_grad\n",
    "\n",
    "    # Print for inspection\n",
    "    print(\"z =\", z, \"| loss =\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do it for a layer of Neuron starting with three Neurons and four Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([1,2,3,4])\n",
    "\n",
    "weights = np.array([[0.1,0.2,0.3,0.4],\n",
    "                    [0.5,0.6,0.7,0.8],\n",
    "                    [0.9,1.0,1.1,1.2]])\n",
    "biases = np.array([0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 466.56000000000006 as 0th iteraton\n",
      "loss = 309.14078976 as 1th iteraton\n",
      "loss = 204.83545073181696 as 2th iteraton\n",
      "loss = 140.8182193826751 as 3th iteraton\n",
      "loss = 108.06052191699968 as 4th iteraton\n",
      "loss = 82.92305106657953 as 5th iteraton\n",
      "loss = 63.63315923526753 as 6th iteraton\n",
      "loss = 48.83055920132266 as 7th iteraton\n",
      "loss = 37.47139919767418 as 8th iteraton\n",
      "loss = 28.754652430714422 as 9th iteraton\n",
      "loss = 22.06563016367191 as 10th iteraton\n",
      "loss = 16.932635012477895 as 11th iteraton\n",
      "loss = 14.840512693931485 as 12th iteraton\n",
      "loss = 13.057336050679451 as 13th iteraton\n",
      "loss = 11.488418780174012 as 14th iteraton\n",
      "loss = 10.108016333223423 as 15th iteraton\n",
      "loss = 8.89347752268863 as 16th iteraton\n",
      "loss = 7.824872837472455 as 17th iteraton\n",
      "loss = 6.884667416813112 as 18th iteraton\n",
      "loss = 6.057433318678514 as 19th iteraton\n",
      "loss = 5.32959636083938 as 20th iteraton\n",
      "loss = 4.689213380506364 as 21th iteraton\n",
      "loss = 4.1257762575582415 as 22th iteraton\n",
      "loss = 3.630039485555074 as 23th iteraton\n",
      "loss = 3.1938684611287185 as 24th iteraton\n",
      "loss = 2.810106002313336 as 25th iteraton\n",
      "loss = 2.472454905499375 as 26th iteraton\n",
      "loss = 2.1753746138741916 as 27th iteraton\n",
      "loss = 1.9139903017695257 as 28th iteraton\n",
      "loss = 1.6840128830701064 as 29th iteraton\n",
      "loss = 1.481668631091935 as 30th iteraton\n",
      "loss = 1.3036372550544522 as 31th iteraton\n",
      "loss = 1.1469974170361295 as 32th iteraton\n",
      "loss = 1.0091787953947362 as 33th iteraton\n",
      "loss = 0.8879199080552862 as 34th iteraton\n",
      "loss = 0.7812310035829951 as 35th iteraton\n",
      "loss = 0.687361411116477 as 36th iteraton\n",
      "loss = 0.6047708134023655 as 37th iteraton\n",
      "loss = 0.5321039715471908 as 38th iteraton\n",
      "loss = 0.4681684867419666 as 39th iteraton\n",
      "loss = 0.41191523404899866 as 40th iteraton\n",
      "loss = 0.362421147186607 as 41th iteraton\n",
      "loss = 0.31887407182525307 as 42th iteraton\n",
      "loss = 0.2805594388510181 as 43th iteraton\n",
      "loss = 0.2468485389164351 as 44th iteraton\n",
      "loss = 0.21718820587439186 as 45th iteraton\n",
      "loss = 0.1910917398093484 as 46th iteraton\n",
      "loss = 0.16813092072081634 as 47th iteraton\n",
      "loss = 0.14792898181068598 as 48th iteraton\n",
      "loss = 0.13015442707224115 as 49th iteraton\n",
      "loss = 0.11451559173294901 as 50th iteraton\n",
      "loss = 0.10075585629268477 as 51th iteraton\n",
      "loss = 0.0886494356239809 as 52th iteraton\n",
      "loss = 0.07799767403714583 as 53th iteraton\n",
      "loss = 0.06862578551553851 as 54th iteraton\n",
      "loss = 0.060379985631133434 as 55th iteraton\n",
      "loss = 0.05312496807763904 as 56th iteraton\n",
      "loss = 0.0467416844133022 as 57th iteraton\n",
      "loss = 0.041125390580937476 as 58th iteraton\n",
      "loss = 0.03618392815029437 as 59th iteraton\n",
      "loss = 0.031836212079467595 as 60th iteraton\n",
      "loss = 0.028010900180847065 as 61th iteraton\n",
      "loss = 0.024645222458717243 as 62th iteraton\n",
      "loss = 0.021683951108967602 as 63th iteraton\n",
      "loss = 0.019078494279518517 as 64th iteraton\n",
      "loss = 0.016786098720868715 as 65th iteraton\n",
      "loss = 0.01476914824296401 as 66th iteraton\n",
      "loss = 0.012994546466682442 as 67th iteraton\n",
      "loss = 0.01143317374143173 as 68th iteraton\n",
      "loss = 0.010059409317356252 as 69th iteraton\n",
      "loss = 0.008850710931420008 as 70th iteraton\n",
      "loss = 0.007787244908744288 as 71th iteraton\n",
      "loss = 0.006851560709489224 as 72th iteraton\n",
      "loss = 0.006028304580879819 as 73th iteraton\n",
      "loss = 0.005303967615659638 as 74th iteraton\n",
      "loss = 0.004666664082832435 as 75th iteraton\n",
      "loss = 0.00410593639329562 as 76th iteraton\n",
      "loss = 0.0036125835000228004 as 77th iteraton\n",
      "loss = 0.003178509916994065 as 78th iteraton\n",
      "loss = 0.0027965928794077147 as 79th iteraton\n",
      "loss = 0.002460565465389601 as 80th iteraton\n",
      "loss = 0.0021649137613302567 as 81th iteraton\n",
      "loss = 0.0019047863834238405 as 82th iteraton\n",
      "loss = 0.0016759148707371741 as 83th iteraton\n",
      "loss = 0.001474543643528874 as 84th iteraton\n",
      "loss = 0.0012973683774970255 as 85th iteraton\n",
      "loss = 0.0011414817827304954 as 86th iteraton\n",
      "loss = 0.0010043258976447365 as 87th iteraton\n",
      "loss = 0.0008836501150873324 as 88th iteraton\n",
      "loss = 0.0007774742518588998 as 89th iteraton\n",
      "loss = 0.0006840560556525385 as 90th iteraton\n",
      "loss = 0.0006018626162295437 as 91th iteraton\n",
      "loss = 0.0005295452117138669 as 92th iteraton\n",
      "loss = 0.00046591717725518237 as 93th iteraton\n",
      "loss = 0.0004099344329049093 as 94th iteraton\n",
      "loss = 0.00036067835118478575 as 95th iteraton\n",
      "loss = 0.00031734068321982587 as 96th iteraton\n",
      "loss = 0.0002792102960868692 as 97th iteraton\n",
      "loss = 0.00024566150375025336 as 98th iteraton\n",
      "loss = 0.0002161438001056386 as 99th iteraton\n",
      "loss = 0.000190172825660145 as 100th iteraton\n",
      "loss = 0.00016732241962012735 as 101th iteraton\n",
      "loss = 0.00014721762696825352 as 102th iteraton\n",
      "loss = 0.00012952854578225566 as 103th iteraton\n",
      "loss = 0.00011396491383524246 as 104th iteraton\n",
      "loss = 0.00010027134564845534 as 105th iteraton\n",
      "loss = 8.822314184071818e-05 as 106th iteraton\n",
      "loss = 7.76226020097066e-05 as 107th iteraton\n",
      "loss = 6.82957806426276e-05 as 108th iteraton\n",
      "loss = 6.008963282373209e-05 as 109th iteraton\n",
      "loss = 5.28695029021632e-05 as 110th iteraton\n",
      "loss = 4.6516914911452744e-05 as 111th iteraton\n",
      "loss = 4.0927628483352786e-05 as 112th iteraton\n",
      "loss = 3.6009928355305675e-05 as 113th iteraton\n",
      "loss = 3.1683119403846986e-05 as 114th iteraton\n",
      "loss = 2.787620250875636e-05 as 115th iteraton\n",
      "loss = 2.4526709520114518e-05 as 116th iteraton\n",
      "loss = 2.157967821101559e-05 as 117th iteraton\n",
      "loss = 1.898675039589321e-05 as 118th iteraton\n",
      "loss = 1.6705378415325086e-05 as 119th iteraton\n",
      "loss = 1.4698126966451542e-05 as 120th iteraton\n",
      "loss = 1.293205882267119e-05 as 121th iteraton\n",
      "loss = 1.1378194362774346e-05 as 122th iteraton\n",
      "loss = 1.0011036040919489e-05 as 123th iteraton\n",
      "loss = 8.80814999438679e-06 as 124th iteraton\n",
      "loss = 7.749797923661431e-06 as 125th iteraton\n",
      "loss = 6.818613204344925e-06 as 126th iteraton\n",
      "loss = 5.999315916163863e-06 as 127th iteraton\n",
      "loss = 5.278462112941122e-06 as 128th iteraton\n",
      "loss = 4.644223219297674e-06 as 129th iteraton\n",
      "loss = 4.086191934160694e-06 as 130th iteraton\n",
      "loss = 3.5952114561194955e-06 as 131th iteraton\n",
      "loss = 3.163225228398182e-06 as 132th iteraton\n",
      "loss = 2.7831447378551217e-06 as 133th iteraton\n",
      "loss = 2.4487331987338138e-06 as 134th iteraton\n",
      "loss = 2.1545032125068455e-06 as 135th iteraton\n",
      "loss = 1.895626724504719e-06 as 136th iteraton\n",
      "loss = 1.6678557997948718e-06 as 137th iteraton\n",
      "loss = 1.4674529183147234e-06 as 138th iteraton\n",
      "loss = 1.2911296454616241e-06 as 139th iteraton\n",
      "loss = 1.1359926717815175e-06 as 140th iteraton\n",
      "loss = 9.994963363109655e-07 as 141th iteraton\n",
      "loss = 8.794008545252129e-07 as 142th iteraton\n",
      "loss = 7.737355654488364e-07 as 143th iteraton\n",
      "loss = 6.807665948467607e-07 as 144th iteraton\n",
      "loss = 5.989684038763183e-07 as 145th iteraton\n",
      "loss = 5.269987563400152e-07 as 146th iteraton\n",
      "loss = 4.636766937733644e-07 as 147th iteraton\n",
      "loss = 4.0796315695611795e-07 as 148th iteraton\n",
      "loss = 3.5894393586886055e-07 as 149th iteraton\n",
      "loss = 3.1581466831063177e-07 as 150th iteraton\n",
      "loss = 2.7786764102513465e-07 as 151th iteraton\n",
      "loss = 2.444801767501443e-07 as 152th iteraton\n",
      "loss = 2.1510441663245662e-07 as 153th iteraton\n",
      "loss = 1.892583303474886e-07 as 154th iteraton\n",
      "loss = 1.6651780640630245e-07 as 155th iteraton\n",
      "loss = 1.4650969285977134e-07 as 156th iteraton\n",
      "loss = 1.289056742045843e-07 as 157th iteraton\n",
      "loss = 1.1341688401476149e-07 as 158th iteraton\n",
      "loss = 9.978916489917351e-08 as 159th iteraton\n",
      "loss = 8.779889800154524e-08 as 160th iteraton\n",
      "loss = 7.72493336133102e-08 as 161th iteraton\n",
      "loss = 6.796736268358103e-08 as 162th iteraton\n",
      "loss = 5.980067625299646e-08 as 163th iteraton\n",
      "loss = 5.261526619713125e-08 as 164th iteraton\n",
      "loss = 4.629322627196534e-08 as 165th iteraton\n",
      "loss = 4.0730817376039655e-08 as 166th iteraton\n",
      "loss = 3.5836765283401785e-08 as 167th iteraton\n",
      "loss = 3.153076291402601e-08 as 168th iteraton\n",
      "loss = 2.77421525653483e-08 as 169th iteraton\n",
      "loss = 2.4408766481736144e-08 as 170th iteraton\n",
      "loss = 2.14759067363383e-08 as 171th iteraton\n",
      "loss = 1.8895447686540504e-08 as 172th iteraton\n",
      "loss = 1.6625046274292638e-08 as 173th iteraton\n",
      "loss = 1.4627447214131197e-08 as 174th iteraton\n",
      "loss = 1.2869871666684987e-08 as 175th iteraton\n",
      "loss = 1.1323479366681083e-08 as 176th iteraton\n",
      "loss = 9.962895379898372e-09 as 177th iteraton\n",
      "loss = 8.765793722617453e-09 as 178th iteraton\n",
      "loss = 7.712531012094228e-09 as 179th iteraton\n",
      "loss = 6.7858241357822796e-09 as 180th iteraton\n",
      "loss = 5.970466650935363e-09 as 181th iteraton\n",
      "loss = 5.253079260026447e-09 as 182th iteraton\n",
      "loss = 4.621890268470638e-09 as 183th iteraton\n",
      "loss = 4.066542421370032e-09 as 184th iteraton\n",
      "loss = 3.5779229501800417e-09 as 185th iteraton\n",
      "loss = 3.1480140401832544e-09 as 186th iteraton\n",
      "loss = 2.769761265155462e-09 as 187th iteraton\n",
      "loss = 2.4369578305909104e-09 as 188th iteraton\n",
      "loss = 2.1441427254992643e-09 as 189th iteraton\n",
      "loss = 1.8865111121738734e-09 as 190th iteraton\n",
      "loss = 1.6598354829854144e-09 as 191th iteraton\n",
      "loss = 1.4603962906971435e-09 as 192th iteraton\n",
      "loss = 1.284920913992261e-09 as 193th iteraton\n",
      "loss = 1.1305299566400613e-09 as 194th iteraton\n",
      "loss = 9.946899991675513e-10 as 195th iteraton\n",
      "loss = 8.751720276272285e-10 as 196th iteraton\n",
      "loss = 7.700148574790648e-10 as 197th iteraton\n",
      "loss = 6.774929522716155e-10 as 198th iteraton\n",
      "loss = 5.960881091030875e-10 as 199th iteraton\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    z = (weights @ inputs) + biases #shape (3,)\n",
    "    relu_out = Relu(z) #shape (3,)\n",
    "    output = np.sum(relu_out) #scalar\n",
    "    loss = (output-true) ** 2\n",
    "    print(f\"loss = {loss} as {i}th iteraton\")\n",
    "    # updation\n",
    "    dl_dout = 2*(output - true) #scalar\n",
    "    dReluout_dz = diff_Relu(z) #shape (3,)\n",
    "    dxiwi_dw = inputs \n",
    "\n",
    "    grad_w = np.outer(dl_dout*dReluout_dz,dxiwi_dw)\n",
    "    grad_b = dl_dout*dReluout_dz\n",
    "    weights = weights - learning_rate*grad_w\n",
    "    biases = biases - learning_rate*grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.04631337, -0.13570281,  3.58201618],\n",
       "       [-3.94631337, -0.03570281,  3.68201618],\n",
       "       [-3.84631337,  0.06429719,  3.78201618]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
