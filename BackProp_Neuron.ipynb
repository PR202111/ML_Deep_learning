{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def diff_Relu(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumming 0 To be the true value of the neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "true = 0\n",
    "w = np.array([-3,-1,2])\n",
    "b = 1\n",
    "x = np.array([1,-2,3])\n",
    "output = Relu(np.sum(np.dot(x,w)) + b)\n",
    "loss = (output-true) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 6 | loss = 36\n",
      "z = 5.82 | loss = 33.872400000000006\n",
      "z = 5.6453999999999995 | loss = 31.870541159999995\n",
      "z = 5.476038000000001 | loss = 29.98699217744401\n",
      "z = 5.31175686 | loss = 28.21476093975706\n",
      "z = 5.1524041542 | loss = 26.54726856821742\n",
      "z = 4.997832029574 | loss = 24.978324995835766\n",
      "z = 4.8478970686867795 | loss = 23.50210598858187\n",
      "z = 4.702460156626176 | loss = 22.113131524656684\n",
      "z = 4.561386351927392 | loss = 20.80624545154948\n",
      "z = 4.424544761369571 | loss = 19.576596345362915\n",
      "z = 4.2918084185284835 | loss = 18.419619501351963\n",
      "z = 4.163054165972628 | loss = 17.331019988822057\n",
      "z = 4.03816254099345 | loss = 16.306756707482677\n",
      "z = 3.9170176647636463 | loss = 15.34302738607045\n",
      "z = 3.799507134820737 | loss = 14.436254467553686\n",
      "z = 3.6855219207761145 | loss = 13.58307182852126\n",
      "z = 3.574956263152831 | loss = 12.780312283455652\n",
      "z = 3.4677075752582462 | loss = 12.024995827503426\n",
      "z = 3.363676348000499 | loss = 11.314318574097975\n",
      "z = 3.262766057560484 | loss = 10.645642346368783\n",
      "z = 3.16488307583367 | loss = 10.016484883698393\n",
      "z = 3.06993658355866 | loss = 9.424510627071816\n",
      "z = 2.9778384860519 | loss = 8.867522049011871\n",
      "z = 2.8885033314703428 | loss = 8.34345149591527\n",
      "z = 2.8018482315262325 | loss = 7.850353512506676\n",
      "z = 2.717792784580446 | loss = 7.386397619917536\n",
      "z = 2.6362590010430327 | loss = 6.949861520580408\n",
      "z = 2.5571712310117416 | loss = 6.539124704714106\n",
      "z = 2.4804560940813896 | loss = 6.152662434665503\n",
      "z = 2.406042411258948 | loss = 5.789040084776773\n",
      "z = 2.3338611389211796 | loss = 5.446907815766465\n",
      "z = 2.263845304753545 | loss = 5.124995563854671\n",
      "z = 2.1959299456109376 | loss = 4.822108326030855\n",
      "z = 2.13005204724261 | loss = 4.537121723962434\n",
      "z = 2.066150485825332 | loss = 4.268977830076255\n",
      "z = 2.004165971250572 | loss = 4.016681240318748\n",
      "z = 1.944040992113055 | loss = 3.7792953790159114\n",
      "z = 1.8857197623496635 | loss = 3.5559390221160716\n",
      "z = 1.8291481694791734 | loss = 3.345783025909011\n",
      "z = 1.7742737243947984 | loss = 3.1480472490777887\n",
      "z = 1.721045512662954 | loss = 2.96199765665729\n",
      "z = 1.6694141472830655 | loss = 2.786943595148845\n",
      "z = 1.6193317228645738 | loss = 2.622235228675549\n",
      "z = 1.5707517711786363 | loss = 2.467261126660823\n",
      "z = 1.523629218043277 | loss = 2.3214459940751673\n",
      "z = 1.4779203415019788 | loss = 2.1842485358253256\n",
      "z = 1.4335827312569185 | loss = 2.0551594473580463\n",
      "z = 1.3905752493192107 | loss = 1.933699524019185\n",
      "z = 1.3488579918396344 | loss = 1.8194178821496512\n",
      "z = 1.3083922520844453 | loss = 1.7118902853146067\n",
      "z = 1.269140484521912 | loss = 1.6107175694525138\n",
      "z = 1.2310662699862545 | loss = 1.5155241610978696\n",
      "z = 1.194134281886667 | loss = 1.4259566831769857\n",
      "z = 1.1583102534300664 | loss = 1.3416826432012248\n",
      "z = 1.123560945827165 | loss = 1.2623891989880334\n",
      "z = 1.08985411745235 | loss = 1.1877819973278405\n",
      "z = 1.0571584939287788 | loss = 1.1175840812857638\n",
      "z = 1.0254437391109161 | loss = 1.0515348620817766\n",
      "z = 0.9946804269375884 | loss = 0.9893891517327431\n",
      "z = 0.964840014129461 | loss = 0.9309162528653384\n",
      "z = 0.9358948137055771 | loss = 0.875899102320997\n",
      "z = 0.9078179692944098 | loss = 0.824133465373826\n",
      "z = 0.8805834302155773 | loss = 0.7754271775702325\n",
      "z = 0.8541659273091098 | loss = 0.7295994313758314\n",
      "z = 0.828540949489836 | loss = 0.686480104981519\n",
      "z = 0.8036847210051412 | loss = 0.6459091307771117\n",
      "z = 0.779574179374987 | loss = 0.6077359011481843\n",
      "z = 0.7561869539937376 | loss = 0.571818709390327\n",
      "z = 0.7335013453739249 | loss = 0.5380242236653578\n",
      "z = 0.7114963050127071 | loss = 0.5062269920467352\n",
      "z = 0.690151415862326 | loss = 0.4763089768167732\n",
      "z = 0.6694468733864556 | loss = 0.4481591162869011\n",
      "z = 0.6493634671848619 | loss = 0.42167291251434524\n",
      "z = 0.629882563169316 | loss = 0.3967520433847474\n",
      "z = 0.6109860862742363 | loss = 0.3733039976207086\n",
      "z = 0.5926565036860091 | loss = 0.35124173136132447\n",
      "z = 0.5748768085754289 | loss = 0.3304833450378703\n",
      "z = 0.5576305043181661 | loss = 0.3109517793461322\n",
      "z = 0.5409015891886207 | loss = 0.29257452918677546\n",
      "z = 0.524674541512962 | loss = 0.2752833745118369\n",
      "z = 0.508934305267573 | loss = 0.25901412707818716\n",
      "z = 0.49366627610954616 | loss = 0.24370639216786666\n",
      "z = 0.47885628782625966 | loss = 0.22930334439074565\n",
      "z = 0.4644905991914724 | loss = 0.21575151673725307\n",
      "z = 0.4505558812157282 | loss = 0.20300060209808138\n",
      "z = 0.4370392047792562 | loss = 0.1910032665140846\n",
      "z = 0.4239280286358783 | loss = 0.17971497346310206\n",
      "z = 0.4112101877768026 | loss = 0.16909381853143327\n",
      "z = 0.39887388214349795 | loss = 0.15910037385622508\n",
      "z = 0.3869076656791933 | loss = 0.14969754176132244\n",
      "z = 0.37530043570881766 | loss = 0.14085041704322837\n",
      "z = 0.364041422637553 | loss = 0.1325261573959735\n",
      "z = 0.35312017995842615 | loss = 0.12469386149387127\n",
      "z = 0.3425265745596734 | loss = 0.11732445427958349\n",
      "z = 0.33225077732288333 | loss = 0.1103905790316602\n",
      "z = 0.3222832540031969 | loss = 0.10386649581088914\n",
      "z = 0.31261475638310077 | loss = 0.09772798590846545\n",
      "z = 0.303236313691608 | loss = 0.09195226194127527\n",
      "z = 0.2941392242808595 | loss = 0.08651788326054576\n",
      "z = 0.285315047552434 | loss = 0.08140467635984766\n",
      "z = 0.27675559612586087 | loss = 0.07659365998698062\n",
      "z = 0.2684529282420851 | loss = 0.07206697468175009\n",
      "z = 0.26039934039482204 | loss = 0.0678078164780584\n",
      "z = 0.25258736018297734 | loss = 0.06380037452420513\n",
      "z = 0.24500973937748782 | loss = 0.06002977238982451\n",
      "z = 0.23765944719616283 | loss = 0.05648201284158571\n",
      "z = 0.23052966378027762 | loss = 0.05314392588264784\n",
      "z = 0.2236137738668692 | loss = 0.050003119862983315\n",
      "z = 0.2169053606508634 | loss = 0.04704793547908113\n",
      "z = 0.21039819983133712 | loss = 0.044267402492267266\n",
      "z = 0.20408625383639667 | loss = 0.04165119900497413\n",
      "z = 0.197963666221305 | loss = 0.03918961314378026\n",
      "z = 0.1920247562346662 | loss = 0.036873507006982977\n",
      "z = 0.18626401354762623 | loss = 0.034694282742870286\n",
      "z = 0.1806760931411977 | loss = 0.03264385063276675\n",
      "z = 0.17525581034696203 | loss = 0.030714599060370322\n",
      "z = 0.16999813603655345 | loss = 0.02889936625590253\n",
      "z = 0.16489819195545652 | loss = 0.027191413710178584\n",
      "z = 0.15995124619679246 | loss = 0.025584401159906914\n",
      "z = 0.15515270881088894 | loss = 0.024072363051356495\n",
      "z = 0.15049812754656233 | loss = 0.022649686395021344\n",
      "z = 0.1459831837201655 | loss = 0.021311089929075593\n",
      "z = 0.14160368820856073 | loss = 0.020051604514267282\n",
      "z = 0.137355577562304 | loss = 0.018866554687474106\n",
      "z = 0.1332349102354351 | loss = 0.01775154130544445\n",
      "z = 0.12923786292837158 | loss = 0.016702425214292563\n",
      "z = 0.12536072704052104 | loss = 0.015715311884128023\n",
      "z = 0.12159990522930542 | loss = 0.014786536951776058\n",
      "z = 0.11795190807242573 | loss = 0.013912652617925968\n",
      "z = 0.11441335083025317 | loss = 0.013090414848206593\n",
      "z = 0.11098095030534594 | loss = 0.012316771330677665\n",
      "z = 0.10765152179618531 | loss = 0.011588850145034562\n",
      "z = 0.10442197614230009 | loss = 0.01090394910146309\n",
      "z = 0.10128931685803055 | loss = 0.010259525709566512\n",
      "z = 0.09825063735229023 | loss = 0.009653187740131247\n",
      "z = 0.09530311823172133 | loss = 0.009082684344689455\n",
      "z = 0.09244402468476909 | loss = 0.008545897699918197\n",
      "z = 0.08967070394422649 | loss = 0.008040835145853117\n",
      "z = 0.08698058282589993 | loss = 0.007565621788733238\n",
      "z = 0.08437116534112299 | loss = 0.007118493541019113\n",
      "z = 0.08184003038088916 | loss = 0.0066977905727448606\n",
      "z = 0.07938482946946301 | loss = 0.0063019511498957235\n",
      "z = 0.07700328458537886 | loss = 0.005929505836936846\n",
      "z = 0.0746931860478176 | loss = 0.005579072041973895\n",
      "z = 0.07245239046638274 | loss = 0.005249348884293189\n",
      "z = 0.07027881875239106 | loss = 0.0049391123652314335\n",
      "z = 0.06817045418981948 | loss = 0.004647210824446277\n",
      "z = 0.06612534056412489 | loss = 0.004372560664721501\n",
      "z = 0.06414158034720152 | loss = 0.004114142329436509\n",
      "z = 0.06221733293678566 | loss = 0.003870996517766834\n",
      "z = 0.06035081294868194 | loss = 0.003642220623566796\n",
      "z = 0.05854028856022153 | loss = 0.003426965384714004\n",
      "z = 0.05678407990341516 | loss = 0.003224431730477438\n",
      "z = 0.055080557506312355 | loss = 0.0030338678152061825\n",
      "z = 0.053428140781123123 | loss = 0.0028545662273275116\n",
      "z = 0.05182529655768919 | loss = 0.002685861363292431\n",
      "z = 0.05027053766095846 | loss = 0.0025271269567218426\n",
      "z = 0.04876242153112986 | loss = 0.0023777737535795973\n",
      "z = 0.04729954888519583 | loss = 0.00223724732474303\n",
      "z = 0.04588056241864025 | loss = 0.002105026007850744\n",
      "z = 0.04450414554608073 | loss = 0.0019806189707867374\n",
      "z = 0.04316902117969834 | loss = 0.001863564389613244\n",
      "z = 0.04187395054430754 | loss = 0.0017534277341871133\n",
      "z = 0.04061773202797825 | loss = 0.00164980015509665\n",
      "z = 0.03939920006713904 | loss = 0.001552296965930449\n",
      "z = 0.038217224065124955 | loss = 0.001460556215243966\n",
      "z = 0.03707070734317086 | loss = 0.001374237342923022\n",
      "z = 0.035958586122875835 | loss = 0.0012930199159562786\n",
      "z = 0.03487982853918947 | loss = 0.0012166024389232565\n",
      "z = 0.03383343368301395 | loss = 0.0011447012347829027\n",
      "z = 0.03281843067252366 | loss = 0.0010770493918072417\n",
      "z = 0.031833877752347584 | loss = 0.0010133957727514104\n",
      "z = 0.030878861419777137 | loss = 0.0009535040825818009\n",
      "z = 0.029952495577183602 | loss = 0.0008971519913012032\n",
      "z = 0.02905392070986801 | loss = 0.0008441303086152972\n",
      "z = 0.02818230308857217 | loss = 0.0007942422073761444\n",
      "z = 0.02733683399591469 | loss = 0.0007473024929201971\n",
      "z = 0.026516728976037407 | loss = 0.0007031369155886218\n",
      "z = 0.025721227106756173 | loss = 0.0006615815238773285\n",
      "z = 0.024949590293553703 | loss = 0.0006224820558161892\n",
      "z = 0.02420110258474739 | loss = 0.0005856933663174669\n",
      "z = 0.023475069507204793 | loss = 0.0005510788883680963\n",
      "z = 0.02277081742198883 | loss = 0.0005185101260655501\n",
      "z = 0.02208769289932877 | loss = 0.00048786617761505856\n",
      "z = 0.021425062112348958 | loss = 0.0004590332865180108\n",
      "z = 0.02078231024897792 | loss = 0.0004319044192847727\n",
      "z = 0.020158840941509104 | loss = 0.0004063788681050637\n",
      "z = 0.01955407571326373 | loss = 0.00038236187700005044\n",
      "z = 0.0189674534418659 | loss = 0.0003597642900693506\n",
      "z = 0.018398429838609953 | loss = 0.0003385022205262531\n",
      "z = 0.01784647694345176 | loss = 0.0003184967392931553\n",
      "z = 0.017311082635148578 | loss = 0.00029967358200094264\n",
      "z = 0.016791750156094354 | loss = 0.00028196287330469475\n",
      "z = 0.01628799765141109 | loss = 0.0002652988674923732\n",
      "z = 0.01579935772186858 | loss = 0.0002496197044235683\n",
      "z = 0.015325376990212192 | loss = 0.0002348671798921253\n",
      "z = 0.014865615680506372 | loss = 0.00022098652956051694\n",
      "z = 0.014419647210091258 | loss = 0.0002079262256634926\n",
      "z = 0.013987057793788504 | loss = 0.00019563778572677975\n"
     ]
    }
   ],
   "source": [
    "for _ in range(200):\n",
    "    z = np.dot(x, w) + b\n",
    "    output = Relu(z)\n",
    "    loss = (output - true) ** 2\n",
    "\n",
    "    dL_dout = 2 * (output - true)\n",
    "    dout_dz = diff_Relu(z)\n",
    "    dz_dw = x\n",
    "    dz_db = 1\n",
    "\n",
    "    # Gradient calculation\n",
    "    gradient = dL_dout * dout_dz * dz_dw\n",
    "    bias_grad = dL_dout * dout_dz * dz_db\n",
    "\n",
    "    # Update weights and bias\n",
    "    w = w - learning_rate * gradient\n",
    "    b = b - learning_rate * bias_grad\n",
    "\n",
    "    # Print for inspection\n",
    "    print(\"z =\", z, \"| loss =\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do it for a layer of Neuron starting with three Neurons and four Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([1,2,3,4])\n",
    "\n",
    "weights = np.array([[0.1,0.2,0.3,0.4],\n",
    "                    [0.5,0.6,0.7,0.8],\n",
    "                    [0.9,1.0,1.1,1.2]])\n",
    "biases = (np.array([0.1,0.2,0.3]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.019490625919571337 as 0th iteraton\n",
      "loss = 0.017148710271579413 as 1th iteraton\n",
      "loss = 0.015088189840187439 as 2th iteraton\n",
      "loss = 0.013275253301749945 as 3th iteraton\n",
      "loss = 0.011680151966024992 as 4th iteraton\n",
      "loss = 0.0102767116263952 as 5th iteraton\n",
      "loss = 0.009041903064214082 as 6th iteraton\n",
      "loss = 0.007955464159630384 as 7th iteraton\n",
      "loss = 0.00699956740806582 as 8th iteraton\n",
      "loss = 0.006158527386582234 as 9th iteraton\n",
      "loss = 0.0054185433699200395 as 10th iteraton\n",
      "loss = 0.0047674728727639835 as 11th iteraton\n",
      "loss = 0.004194632402264156 as 12th iteraton\n",
      "loss = 0.0036906221513377272 as 13th iteraton\n",
      "loss = 0.003247171756121523 as 14th iteraton\n",
      "loss = 0.0028570045865930005 as 15th iteraton\n",
      "loss = 0.0025137183434863295 as 16th iteraton\n",
      "loss = 0.0022116800022064106 as 17th iteraton\n",
      "loss = 0.001945933379861317 as 18th iteraton\n",
      "loss = 0.0017121178086706814 as 19th iteraton\n",
      "loss = 0.0015063965812520124 as 20th iteraton\n",
      "loss = 0.0013253939936351208 as 21th iteraton\n",
      "loss = 0.0011661399529358878 as 22th iteraton\n",
      "loss = 0.0010260212407509164 as 23th iteraton\n",
      "loss = 0.0009027386325472478 as 24th iteraton\n",
      "loss = 0.0007942691694149064 as 25th iteraton\n",
      "loss = 0.0006988329630946797 as 26th iteraton\n",
      "loss = 0.0006148639895810753 as 27th iteraton\n",
      "loss = 0.0005409843920489889 as 28th iteraton\n",
      "loss = 0.00047598187143793315 as 29th iteraton\n",
      "loss = 0.0004187897936934349 as 30th iteraton\n",
      "loss = 0.00036846968724241284 as 31th iteraton\n",
      "loss = 0.0003241958435021196 as 32th iteraton\n",
      "loss = 0.0002852417677302805 as 33th iteraton\n",
      "loss = 0.00025096825788687593 as 34th iteraton\n",
      "loss = 0.00022081291589221298 as 35th iteraton\n",
      "loss = 0.00019428091917027856 as 36th iteraton\n",
      "loss = 0.00017093690104646114 as 37th iteraton\n",
      "loss = 0.0001503978067643228 as 38th iteraton\n",
      "loss = 0.00013232660789474715 as 39th iteraton\n",
      "loss = 0.00011642677199654705 as 40th iteraton\n",
      "loss = 0.0001024373967805228 as 41th iteraton\n",
      "loss = 9.012892893296966e-05 as 42th iteraton\n",
      "loss = 7.929939734810527e-05 as 43th iteraton\n",
      "loss = 6.977109896034035e-05 as 44th iteraton\n",
      "loss = 6.138768279366503e-05 as 45th iteraton\n",
      "loss = 5.401158437990196e-05 as 46th iteraton\n",
      "loss = 4.752176844715018e-05 as 47th iteraton\n",
      "loss = 4.18117428376116e-05 as 48th iteraton\n",
      "loss = 3.6787811065215584e-05 as 49th iteraton\n",
      "loss = 3.2367534838862994e-05 as 50th iteraton\n",
      "loss = 2.847838132277153e-05 as 51th iteraton\n",
      "loss = 2.505653293655186e-05 as 52th iteraton\n",
      "loss = 2.20458401650297e-05 as 53th iteraton\n",
      "loss = 1.9396900194153837e-05 as 54th iteraton\n",
      "loss = 1.7066246254423525e-05 as 55th iteraton\n",
      "loss = 1.5015634369478498e-05 as 56th iteraton\n",
      "loss = 1.3211415806180648e-05 as 57th iteraton\n",
      "loss = 1.1623984928573049e-05 as 58th iteraton\n",
      "loss = 1.0227293395498379e-05 as 59th iteraton\n",
      "loss = 8.998422730265529e-06 as 60th iteraton\n",
      "loss = 7.917208248687325e-06 as 61th iteraton\n",
      "loss = 6.965908174360742e-06 as 62th iteraton\n",
      "loss = 6.128912511760634e-06 as 63th iteraton\n",
      "loss = 5.392486899995968e-06 as 64th iteraton\n",
      "loss = 4.744547244040788e-06 as 65th iteraton\n",
      "loss = 4.174461425384259e-06 as 66th iteraton\n",
      "loss = 3.672874838356094e-06 as 67th iteraton\n",
      "loss = 3.23155688927863e-06 as 68th iteraton\n",
      "loss = 2.8432659396909135e-06 as 69th iteraton\n",
      "loss = 2.5016304774423505e-06 as 70th iteraton\n",
      "loss = 2.2010445657950984e-06 as 71th iteraton\n",
      "loss = 1.9365758549473683e-06 as 72th iteraton\n",
      "loss = 1.703884646519584e-06 as 73th iteraton\n",
      "loss = 1.499152682932079e-06 as 74th iteraton\n",
      "loss = 1.3190204931623062e-06 as 75th iteraton\n",
      "loss = 1.160532266786721e-06 as 76th iteraton\n",
      "loss = 1.0210873517388033e-06 as 77th iteraton\n",
      "loss = 8.98397579903402e-07 as 78th iteraton\n",
      "loss = 7.904497202923693e-07 as 79th iteraton\n",
      "loss = 6.954724437007668e-07 as 80th iteraton\n",
      "loss = 6.119072567556812e-07 as 81th iteraton\n",
      "loss = 5.383829284128765e-07 as 82th iteraton\n",
      "loss = 4.73692989266472e-07 as 83th iteraton\n",
      "loss = 4.167759344487042e-07 as 84th iteraton\n",
      "loss = 3.666978052692696e-07 as 85th iteraton\n",
      "loss = 3.226368637788943e-07 as 86th iteraton\n",
      "loss = 2.83870108774474e-07 as 87th iteraton\n",
      "loss = 2.4976141198468815e-07 as 88th iteraton\n",
      "loss = 2.197510797664067e-07 as 89th iteraton\n",
      "loss = 1.9334666902633019e-07 as 90th iteraton\n",
      "loss = 1.7011490666262664e-07 as 91th iteraton\n",
      "loss = 1.496745799378157e-07 as 92th iteraton\n",
      "loss = 1.3169028111095192e-07 as 93th iteraton\n",
      "loss = 1.1586690369318092e-07 as 94th iteraton\n",
      "loss = 1.0194480001339798e-07 as 95th iteraton\n",
      "loss = 8.969552062291206e-08 as 96th iteraton\n",
      "loss = 7.891806564691649e-08 as 97th iteraton\n",
      "loss = 6.943558655087849e-08 as 98th iteraton\n",
      "loss = 6.109248421342304e-08 as 99th iteraton\n",
      "loss = 5.375185567994963e-08 as 100th iteraton\n",
      "loss = 4.729324770906044e-08 as 101th iteraton\n",
      "loss = 4.1610680237176576e-08 as 102th iteraton\n",
      "loss = 3.6610907342744195e-08 as 103th iteraton\n",
      "loss = 3.22118871600978e-08 as 104th iteraton\n",
      "loss = 2.8341435646460378e-08 as 105th iteraton\n",
      "loss = 2.4936042105077163e-08 as 106th iteraton\n",
      "loss = 2.1939827029753218e-08 as 107th iteraton\n",
      "loss = 1.9303625173250843e-08 as 108th iteraton\n",
      "loss = 1.6984178786876534e-08 as 109th iteraton\n",
      "loss = 1.4943427800605765e-08 as 110th iteraton\n",
      "loss = 1.3147885289692707e-08 as 111th iteraton\n",
      "loss = 1.1568087984858015e-08 as 112th iteraton\n",
      "loss = 1.0178112804928269e-08 as 113th iteraton\n",
      "loss = 8.955151482776626e-09 as 114th iteraton\n",
      "loss = 7.879136301164183e-09 as 115th iteraton\n",
      "loss = 6.932410799741386e-09 as 116th iteraton\n",
      "loss = 6.0994400477023654e-09 as 117th iteraton\n",
      "loss = 5.366555729354974e-09 as 118th iteraton\n",
      "loss = 4.721731859070732e-09 as 119th iteraton\n",
      "loss = 4.15438744591928e-09 as 120th iteraton\n",
      "loss = 3.6552128679201495e-09 as 121th iteraton\n",
      "loss = 3.2160171104999798e-09 as 122th iteraton\n",
      "loss = 2.829593358587469e-09 as 123th iteraton\n",
      "loss = 2.489600739063053e-09 as 124th iteraton\n",
      "loss = 2.19046027263176e-09 as 125th iteraton\n",
      "loss = 1.927263328100785e-09 as 126th iteraton\n",
      "loss = 1.6956910756789857e-09 as 127th iteraton\n",
      "loss = 1.4919436187556694e-09 as 128th iteraton\n",
      "loss = 1.3126776412999483e-09 as 129th iteraton\n",
      "loss = 1.1549515466503242e-09 as 130th iteraton\n",
      "loss = 1.0161771885888106e-09 as 131th iteraton\n",
      "loss = 8.940774023347925e-10 as 132th iteraton\n",
      "loss = 7.866486379789066e-10 as 133th iteraton\n",
      "loss = 6.921280842137712e-10 as 134th iteraton\n",
      "loss = 6.08964742145173e-10 as 135th iteraton\n",
      "loss = 5.357939745779038e-10 as 136th iteraton\n",
      "loss = 4.71415113763893e-10 as 137th iteraton\n",
      "loss = 4.1477175934514493e-10 as 138th iteraton\n",
      "loss = 3.6493444383188263e-10 as 139th iteraton\n",
      "loss = 3.210853808009197e-10 as 140th iteraton\n",
      "loss = 2.82505045788211e-10 as 141th iteraton\n",
      "loss = 2.485603695073509e-10 as 142th iteraton\n",
      "loss = 2.1869434975355409e-10 as 143th iteraton\n",
      "loss = 1.9241691146146132e-10 as 144th iteraton\n",
      "loss = 1.6929686505332952e-10 as 145th iteraton\n",
      "loss = 1.4895483093277303e-10 as 146th iteraton\n",
      "loss = 1.3105701427193266e-10 as 147th iteraton\n",
      "loss = 1.1530972766015297e-10 as 148th iteraton\n",
      "loss = 1.0145457201760464e-10 as 149th iteraton\n",
      "loss = 8.926419646796354e-11 as 150th iteraton\n",
      "loss = 7.853856767555319e-11 as 151th iteraton\n",
      "loss = 6.910168753379483e-11 as 152th iteraton\n",
      "loss = 6.079870517309105e-11 as 153th iteraton\n",
      "loss = 5.3493375952572176e-11 as 154th iteraton\n",
      "loss = 4.7065825868872925e-11 as 155th iteraton\n",
      "loss = 4.1410584499670615e-11 as 156th iteraton\n",
      "loss = 3.643485430942887e-11 as 157th iteraton\n",
      "loss = 3.2056987951957586e-11 as 158th iteraton\n",
      "loss = 2.8205148507838017e-11 as 159th iteraton\n",
      "loss = 2.4816130682464818e-11 as 160th iteraton\n",
      "loss = 2.1834323684506292e-11 as 161th iteraton\n",
      "loss = 1.9210798688516975e-11 as 162th iteraton\n",
      "loss = 1.6902505958086177e-11 as 163th iteraton\n",
      "loss = 1.4871568459173664e-11 as 164th iteraton\n",
      "loss = 1.3084660273237499e-11 as 165th iteraton\n",
      "loss = 1.1512459837370069e-11 as 166th iteraton\n",
      "loss = 1.0129168714756624e-11 as 167th iteraton\n",
      "loss = 8.91208831697998e-12 as 168th iteraton\n",
      "loss = 7.841247432886378e-12 as 169th iteraton\n",
      "loss = 6.899074505687272e-12 as 170th iteraton\n",
      "loss = 6.070109309093062e-12 as 171th iteraton\n",
      "loss = 5.340749254982517e-12 as 172th iteraton\n",
      "loss = 4.6990261882132085e-12 as 173th iteraton\n",
      "loss = 4.134409999098996e-12 as 174th iteraton\n",
      "loss = 3.637635831206601e-12 as 175th iteraton\n",
      "loss = 3.2005520598081646e-12 as 176th iteraton\n",
      "loss = 2.815986527505471e-12 as 177th iteraton\n",
      "loss = 2.477628850868534e-12 as 178th iteraton\n",
      "loss = 2.179926877276158e-12 as 179th iteraton\n",
      "loss = 1.9179955832896188e-12 as 180th iteraton\n",
      "loss = 1.687536908009927e-12 as 181th iteraton\n",
      "loss = 1.4847692218690027e-12 as 182th iteraton\n",
      "loss = 1.3063652909314127e-12 as 183th iteraton\n",
      "loss = 1.1493976641750125e-12 as 184th iteraton\n",
      "loss = 1.011290637914997e-12 as 185th iteraton\n",
      "loss = 8.897780010159638e-13 as 186th iteraton\n",
      "loss = 7.828658343533901e-13 as 187th iteraton\n",
      "loss = 6.887998069853861e-13 as 188th iteraton\n",
      "loss = 6.060363770564256e-13 as 189th iteraton\n",
      "loss = 5.332174705447257e-13 as 190th iteraton\n",
      "loss = 4.691481921357031e-13 as 191th iteraton\n",
      "loss = 4.127772217126489e-13 as 192th iteraton\n",
      "loss = 3.631795616903327e-13 as 193th iteraton\n",
      "loss = 3.1954135806399577e-13 as 194th iteraton\n",
      "loss = 2.811465465775847e-13 as 195th iteraton\n",
      "loss = 2.4736510220210467e-13 as 196th iteraton\n",
      "loss = 2.17642700789648e-13 as 197th iteraton\n",
      "loss = 1.9149162448798013e-13 as 198th iteraton\n",
      "loss = 1.6848275665476113e-13 as 199th iteraton\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    z = weights @ inputs + biases #shape (3,)\n",
    "    sum_z = np.sum(z,axis =1) #shape (3,)\n",
    "    relu_out = Relu(sum_z) #shape (3,)\n",
    "    output = np.sum(relu_out) #scalar\n",
    "    loss = (output-true) ** 2\n",
    "    print(f\"loss = {loss} as {i}th iteraton\")\n",
    "    # updation\n",
    "    dl_dout = 2*(output - true) #scalar\n",
    "    dReluout_dz = diff_Relu(sum_z) #shape (3,)\n",
    "    dxiwi_dw = inputs \n",
    "\n",
    "    grad_w = np.outer(dl_dout*dReluout_dz,dxiwi_dw)\n",
    "    grad_b = dl_dout*dReluout_dz\n",
    "    weights = weights - learning_rate*grad_w\n",
    "    biases = biases - learning_rate*grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
