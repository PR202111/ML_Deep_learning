{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "X,y = spiral_data(100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0][y == 0], X[:, 1][y == 0], c='blue', label='Class 0')\n",
    "plt.scatter(X[:, 0][y == 1], X[:, 1][y == 1], c='red', label='Class 1')\n",
    "plt.scatter(X[:, 0][y == 2], X[:, 1][y == 2], c='green', label='Class 2')\n",
    "plt.xlabel('X - Axis')\n",
    "plt.ylabel('Y - Axis')\n",
    "plt.title('Plotting the data points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,no_of_inputs ,no_of_neurons):\n",
    "        self.weights = np.random.randn(no_of_inputs, no_of_neurons) * 0.01\n",
    "        self.biases = np.zeros((1, no_of_neurons))\n",
    "        self.weights_momentum = np.zeros_like(self.weights)\n",
    "        self.biases_momentum = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class ReluActivation:\n",
    "    def forward(self, dense_output):\n",
    "        self.inputs = dense_output\n",
    "        self.output = np.maximum(0, dense_output)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs < 0] = 0\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_inputs = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        sum_exp = np.sum(exp_inputs, axis=1, keepdims=True)\n",
    "        self.output = exp_inputs / sum_exp\n",
    "        return self.output\n",
    "\n",
    "\n",
    "class CrossEntLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            return -np.mean(np.log(y_pred[range(samples), y_true]))\n",
    "        elif len(y_true.shape) == 2:\n",
    "            return -np.mean(np.log(np.sum(y_pred * y_true, axis=1)))\n",
    "\n",
    "\n",
    "class Softmax_CrossentLoss:\n",
    "    def __init__(self):\n",
    "        self.activation = Softmax()\n",
    "        self.loss = CrossEntLoss()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.softmax_output = self.activation.forward(inputs)\n",
    "        return self.loss.forward(self.softmax_output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs /= samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_GD:\n",
    "    def __init__(self,learning_rate = 1):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update_param(self,layer):\n",
    "        layer.weights -= self.learning_rate*layer.dweights\n",
    "        layer.biases -= self.learning_rate*layer.dbiases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(100,3)\n",
    "\n",
    "dense1 = Dense(2,64)\n",
    "activation1 = ReluActivation()\n",
    "\n",
    "dense2 = Dense(64,3)\n",
    "softmax_loss = Softmax_CrossentLoss()\n",
    "\n",
    "optimizer = Optimizer_GD()\n",
    "\n",
    "for epochs in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = softmax_loss.forward(dense2.output,y)\n",
    "\n",
    "    prediction = np.argmax(softmax_loss.softmax_output,axis=1)\n",
    "    if(len(y.shape) == 2):\n",
    "        y = np.argmax(y,axis=1)\n",
    "\n",
    "    accuracy = np.mean(prediction == y)\n",
    "    if epochs % 100 == 0:\n",
    "        print(f'epochs = {epochs}, loss = {loss:.3f}, acc = {accuracy:.3f}')\n",
    "\n",
    "\n",
    "    softmax_loss.backward(softmax_loss.softmax_output,y)\n",
    "    dense2.backward(softmax_loss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.update_param(dense1)\n",
    "    optimizer.update_param(dense2)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here our loss got stuck so we have to add decay to this grad descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will add the decay rate on our Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    def __init__(self,learning_rate = 1,decay=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iteration = 0\n",
    "        self.current_learning_rate = learning_rate\n",
    "\n",
    "    def update_param(self,layer):\n",
    "        layer.weights -= self.learning_rate*layer.dweights\n",
    "        layer.biases -= self.learning_rate*layer.dbiases\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = (self.learning_rate)/( 1+ self.decay*self.iteration)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(100,3)\n",
    "dense1 = Dense(2,64)\n",
    "activation1 = ReluActivation()\n",
    "\n",
    "dense2 = Dense(64,3)\n",
    "softmax_loss = Softmax_CrossentLoss()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "for epochs in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = softmax_loss.forward(dense2.output,y)\n",
    "\n",
    "    prediction = np.argmax(softmax_loss.softmax_output,axis=1)\n",
    "    if(len(y.shape) == 2):\n",
    "        y = np.argmax(y,axis=1)\n",
    "\n",
    "    accuracy = np.mean(prediction == y)\n",
    "    if epochs % 100 == 0:\n",
    "        print(f'epochs = {epochs}, loss = {loss:.3f}, acc = {accuracy:.3f}')\n",
    "\n",
    "\n",
    "    softmax_loss.backward(softmax_loss.softmax_output,y)\n",
    "    dense2.backward(softmax_loss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_param(dense1)\n",
    "    optimizer.update_param(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Mometum to reduce the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    def __init__(self,learning_rate = 1,decay=0,momentum = 0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iteration = 0\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def update_param(self,layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer,'weights_momentum'):\n",
    "                layer.weights_momentum = np.zeros_like(layer.weights)\n",
    "                layer.biases_momentum = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_update = self.momentum*layer.weights_momentum - self.current_learning_rate * layer.dweights\n",
    "            biases_update = self.momentum*layer.biases_momentum - self.current_learning_rate * layer.dbiases\n",
    "\n",
    "            layer.weights_momentum = weight_update\n",
    "            layer.biases_momentum = biases_update\n",
    "\n",
    "        else:\n",
    "            weight_update =  - self.current_learning_rate * layer.dweights\n",
    "            biases_update = - self.current_learning_rate * layer.dbiases\n",
    "                \n",
    "        layer.weights += weight_update\n",
    "        layer.biases += biases_update\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = (self.learning_rate)/( 1+ self.decay*self.iteration)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(100,3)\n",
    "dense1 = Dense(2,64)\n",
    "activation1 = ReluActivation()\n",
    "\n",
    "dense2 = Dense(64,3)\n",
    "softmax_loss = Softmax_CrossentLoss()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay=1e-3,momentum=0.9)\n",
    "\n",
    "for epochs in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = softmax_loss.forward(dense2.output,y)\n",
    "\n",
    "    prediction = np.argmax(softmax_loss.softmax_output,axis=1)\n",
    "    if(len(y.shape) == 2):\n",
    "        y = np.argmax(y,axis=1)\n",
    "\n",
    "    accuracy = np.mean(prediction == y)\n",
    "    if epochs % 100 == 0:\n",
    "        print(f'epochs = {epochs}, loss = {loss:.3f}, acc = {accuracy:.3f}, lr = {optimizer.current_learning_rate}')\n",
    "\n",
    "\n",
    "    softmax_loss.backward(softmax_loss.softmax_output,y)\n",
    "    dense2.backward(softmax_loss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_param(dense1)\n",
    "    optimizer.update_param(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using AGDAGRAD optimizer to update individual weights with different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_ADAGRAD:\n",
    "    def __init__(self,learning_rate = 1,decay = 0,epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        self.current_learning_rate = self.learning_rate / (1 + self.decay*self.iterations)\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weights_cache'):\n",
    "                layer.weights_cache = np.zeros_like(layer.weights)\n",
    "                layer.biases_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weights_cache += layer.dweights ** 2\n",
    "        layer.biases_cache += layer.dbiases ** 2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate*layer.dweights / (np.sqrt(layer.weights_cache + self.epsilon))\n",
    "        layer.biases += -self.current_learning_rate*layer.dbiases / (np.sqrt(layer.biases_cache + self.epsilon))\n",
    "\n",
    "    def post_update_params(self):\n",
    "         self.iterations += 1\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(100,3)\n",
    "dense1 = Dense(2,64)\n",
    "activation1 = ReluActivation()\n",
    "\n",
    "dense2 = Dense(64,3)\n",
    "softmax_loss = Softmax_CrossentLoss()\n",
    "\n",
    "optimizer = Optimizer_ADAGRAD(decay=1e-3)\n",
    "\n",
    "for epochs in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = softmax_loss.forward(dense2.output,y)\n",
    "\n",
    "    prediction = np.argmax(softmax_loss.softmax_output,axis=1)\n",
    "    if(len(y.shape) == 2):\n",
    "        y = np.argmax(y,axis=1)\n",
    "\n",
    "    accuracy = np.mean(prediction == y)\n",
    "    if epochs % 100 == 0:\n",
    "        print(f'epochs = {epochs}, loss = {loss:.3f}, acc = {accuracy:.3f}, lr = {optimizer.current_learning_rate}')\n",
    "\n",
    "\n",
    "    softmax_loss.backward(softmax_loss.softmax_output,y)\n",
    "    dense2.backward(softmax_loss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets Add RMSprop optimizer as ADAGRAD dec the learning too quickly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMS:\n",
    "    def __init__(self,decay = 0 ,rho=0.9,learning_rate = 0.001,epsilon = 1e-7):\n",
    "        self.decay = decay\n",
    "        self.rho = rho\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        self.current_learning_rate = self.learning_rate/(1 + self.decay * self.iterations)\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer,'weights_cache'):\n",
    "            layer.weights_cache = np.zeros_like(layer.weights)\n",
    "            layer.biases_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weights_cache = self.rho * layer.weights_cache + (1-self.rho)*(layer.dweights ** 2)\n",
    "        layer.biases_cache = self.rho * layer.biases_cache + (1-self.rho)*(layer.dbiases ** 2)\n",
    "\n",
    "        layer.weights -= self.current_learning_rate * layer.dweights /(np.sqrt(layer.weights_cache) + self.epsilon)\n",
    "        layer.biases -= self.current_learning_rate * layer.dbiases /(np.sqrt(layer.biases_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(100,3)\n",
    "\n",
    "dense1 = Dense(2,64)\n",
    "activation1 = ReluActivation()\n",
    "\n",
    "dense2 = Dense(64,3)\n",
    "softmax_loss = Softmax_CrossentLoss()\n",
    "\n",
    "optimizer = Optimizer_RMS(learning_rate=0.02,decay=1e-3,rho=0.999)\n",
    "\n",
    "for epochs in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = softmax_loss.forward(dense2.output,y)\n",
    "\n",
    "    prediction = np.argmax(softmax_loss.softmax_output,axis=1)\n",
    "    if(len(y.shape) == 2):\n",
    "        y = np.argmax(y,axis=1)\n",
    "\n",
    "    accuracy = np.mean(prediction == y)\n",
    "    if epochs % 100 == 0:\n",
    "        print(f'epochs = {epochs}, loss = {loss:.3f}, acc = {accuracy:.3f}, lr = {optimizer.current_learning_rate}')\n",
    "\n",
    "\n",
    "    softmax_loss.backward(softmax_loss.softmax_output,y)\n",
    "    dense2.backward(softmax_loss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer -> we took adaptive learning rate from Rmsprop and momentum term from the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_ADAM:\n",
    "    def __init__(self,learning_rate = 0.001,decay = 0,epsilon = 1e-7,beta1 = 0.9,beta2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        self.current_learning_rate = self.learning_rate / (1 + self.decay*self.iterations)\n",
    "\n",
    "    def update_params(self,layer):\n",
    "    \n",
    "        if not hasattr(layer,'weights_momentum'):\n",
    "            layer.weights_momentum = np.zeros_like(layer.weights)\n",
    "            layer.biases_momentum = np.zeros_like(layer.biases)\n",
    "        \n",
    "        if not hasattr(layer,'weights_cache'):\n",
    "            layer.weights_cache = np.zeros_like(layer.weights)\n",
    "            layer.biases_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        t = self.iterations +1\n",
    "\n",
    "        layer.weights_momentum = self.beta1*layer.weights_momentum + (1-self.beta1)*layer.dweights\n",
    "        layer.biases_momentum = self.beta1*layer.biases_momentum + (1-self.beta1)*layer.dbiases\n",
    "\n",
    "        weight_momentum_corrected = layer.weights_momentum / ( 1- self.beta1 ** t)\n",
    "        biases_momentum_corrected = layer.biases_momentum / ( 1- self.beta1 ** t)\n",
    "\n",
    "        layer.weights_cache = self.beta2 * layer.weights_cache + (1 - self.beta2)*(layer.dweights ** 2)\n",
    "        layer.biases_cache = self.beta2 * layer.biases_cache + (1 - self.beta2)*(layer.dbiases ** 2)\n",
    "\n",
    "\n",
    "        weight_cache_corrected = layer.weights_cache / ( 1- self.beta2 ** t)\n",
    "        biases_cache_corrected = layer.biases_cache / ( 1- self.beta2 ** t)\n",
    "\n",
    "\n",
    "        layer.weights -= (self.current_learning_rate*weight_momentum_corrected) / \\\n",
    "                          (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases -= (self.current_learning_rate*biases_momentum_corrected) / \\\n",
    "                         (np.sqrt(biases_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 0, loss = 1.099, acc = 0.357, lr = 0.02\n",
      "epochs = 100, loss = 0.951, acc = 0.577, lr = 0.01998021958261321\n",
      "epochs = 200, loss = 0.751, acc = 0.730, lr = 0.019960279044701046\n",
      "epochs = 300, loss = 0.619, acc = 0.767, lr = 0.019940378268975763\n",
      "epochs = 400, loss = 0.543, acc = 0.817, lr = 0.01992051713662487\n",
      "epochs = 500, loss = 0.493, acc = 0.820, lr = 0.01990069552930875\n",
      "epochs = 600, loss = 0.447, acc = 0.843, lr = 0.019880913329158343\n",
      "epochs = 700, loss = 0.417, acc = 0.847, lr = 0.019861170418772778\n",
      "epochs = 800, loss = 0.390, acc = 0.860, lr = 0.019841466681217078\n",
      "epochs = 900, loss = 0.367, acc = 0.870, lr = 0.01982180200001982\n",
      "epochs = 1000, loss = 0.349, acc = 0.880, lr = 0.019802176259170884\n",
      "epochs = 1100, loss = 0.333, acc = 0.887, lr = 0.01978258934311912\n",
      "epochs = 1200, loss = 0.324, acc = 0.883, lr = 0.01976304113677013\n",
      "epochs = 1300, loss = 0.310, acc = 0.900, lr = 0.019743531525483964\n",
      "epochs = 1400, loss = 0.297, acc = 0.897, lr = 0.01972406039507293\n",
      "epochs = 1500, loss = 0.287, acc = 0.900, lr = 0.019704627631799327\n",
      "epochs = 1600, loss = 0.285, acc = 0.893, lr = 0.019685233122373254\n",
      "epochs = 1700, loss = 0.275, acc = 0.903, lr = 0.019665876753950384\n",
      "epochs = 1800, loss = 0.265, acc = 0.917, lr = 0.019646558414129805\n",
      "epochs = 1900, loss = 0.259, acc = 0.917, lr = 0.019627277990951823\n",
      "epochs = 2000, loss = 0.251, acc = 0.920, lr = 0.019608035372895814\n",
      "epochs = 2100, loss = 0.254, acc = 0.903, lr = 0.019588830448878047\n",
      "epochs = 2200, loss = 0.242, acc = 0.917, lr = 0.019569663108249594\n",
      "epochs = 2300, loss = 0.237, acc = 0.927, lr = 0.019550533240794143\n",
      "epochs = 2400, loss = 0.235, acc = 0.920, lr = 0.019531440736725945\n",
      "epochs = 2500, loss = 0.231, acc = 0.923, lr = 0.019512385486687673\n",
      "epochs = 2600, loss = 0.224, acc = 0.933, lr = 0.01949336738174836\n",
      "epochs = 2700, loss = 0.226, acc = 0.907, lr = 0.019474386313401298\n",
      "epochs = 2800, loss = 0.217, acc = 0.930, lr = 0.019455442173562\n",
      "epochs = 2900, loss = 0.213, acc = 0.933, lr = 0.019436534854566128\n",
      "epochs = 3000, loss = 0.210, acc = 0.923, lr = 0.01941766424916747\n",
      "epochs = 3100, loss = 0.206, acc = 0.940, lr = 0.019398830250535893\n",
      "epochs = 3200, loss = 0.202, acc = 0.937, lr = 0.019380032752255354\n",
      "epochs = 3300, loss = 0.201, acc = 0.940, lr = 0.01936127164832186\n",
      "epochs = 3400, loss = 0.197, acc = 0.943, lr = 0.01934254683314152\n",
      "epochs = 3500, loss = 0.193, acc = 0.940, lr = 0.019323858201528515\n",
      "epochs = 3600, loss = 0.191, acc = 0.937, lr = 0.019305205648703173\n",
      "epochs = 3700, loss = 0.191, acc = 0.930, lr = 0.01928658907028997\n",
      "epochs = 3800, loss = 0.186, acc = 0.943, lr = 0.01926800836231563\n",
      "epochs = 3900, loss = 0.184, acc = 0.943, lr = 0.019249463421207133\n",
      "epochs = 4000, loss = 0.182, acc = 0.943, lr = 0.019230954143789846\n",
      "epochs = 4100, loss = 0.180, acc = 0.947, lr = 0.019212480427285565\n",
      "epochs = 4200, loss = 0.178, acc = 0.947, lr = 0.019194042169310647\n",
      "epochs = 4300, loss = 0.176, acc = 0.947, lr = 0.019175639267874092\n",
      "epochs = 4400, loss = 0.170, acc = 0.947, lr = 0.019157271621375684\n",
      "epochs = 4500, loss = 0.170, acc = 0.943, lr = 0.0191389391286041\n",
      "epochs = 4600, loss = 0.168, acc = 0.943, lr = 0.019120641688735073\n",
      "epochs = 4700, loss = 0.164, acc = 0.940, lr = 0.019102379201329525\n",
      "epochs = 4800, loss = 0.161, acc = 0.953, lr = 0.01908415156633174\n",
      "epochs = 4900, loss = 0.159, acc = 0.953, lr = 0.01906595868406753\n",
      "epochs = 5000, loss = 0.157, acc = 0.953, lr = 0.01904780045524243\n",
      "epochs = 5100, loss = 0.155, acc = 0.953, lr = 0.019029676780939874\n",
      "epochs = 5200, loss = 0.157, acc = 0.947, lr = 0.019011587562619416\n",
      "epochs = 5300, loss = 0.153, acc = 0.953, lr = 0.01899353270211493\n",
      "epochs = 5400, loss = 0.151, acc = 0.950, lr = 0.018975512101632844\n",
      "epochs = 5500, loss = 0.150, acc = 0.953, lr = 0.018957525663750367\n",
      "epochs = 5600, loss = 0.149, acc = 0.953, lr = 0.018939573291413745\n",
      "epochs = 5700, loss = 0.148, acc = 0.953, lr = 0.018921654887936498\n",
      "epochs = 5800, loss = 0.155, acc = 0.947, lr = 0.018903770356997703\n",
      "epochs = 5900, loss = 0.145, acc = 0.953, lr = 0.01888591960264025\n",
      "epochs = 6000, loss = 0.144, acc = 0.947, lr = 0.018868102529269144\n",
      "epochs = 6100, loss = 0.143, acc = 0.950, lr = 0.018850319041649778\n",
      "epochs = 6200, loss = 0.142, acc = 0.950, lr = 0.018832569044906263\n",
      "epochs = 6300, loss = 0.143, acc = 0.950, lr = 0.018814852444519702\n",
      "epochs = 6400, loss = 0.141, acc = 0.950, lr = 0.018797169146326564\n",
      "epochs = 6500, loss = 0.139, acc = 0.947, lr = 0.018779519056516963\n",
      "epochs = 6600, loss = 0.149, acc = 0.947, lr = 0.018761902081633038\n",
      "epochs = 6700, loss = 0.137, acc = 0.947, lr = 0.018744318128567278\n",
      "epochs = 6800, loss = 0.137, acc = 0.950, lr = 0.018726767104560903\n",
      "epochs = 6900, loss = 0.137, acc = 0.947, lr = 0.018709248917202218\n",
      "epochs = 7000, loss = 0.137, acc = 0.953, lr = 0.018691763474424996\n",
      "epochs = 7100, loss = 0.134, acc = 0.953, lr = 0.018674310684506857\n",
      "epochs = 7200, loss = 0.134, acc = 0.953, lr = 0.018656890456067686\n",
      "epochs = 7300, loss = 0.133, acc = 0.953, lr = 0.01863950269806802\n",
      "epochs = 7400, loss = 0.132, acc = 0.953, lr = 0.018622147319807447\n",
      "epochs = 7500, loss = 0.132, acc = 0.953, lr = 0.018604824230923078\n",
      "epochs = 7600, loss = 0.133, acc = 0.953, lr = 0.01858753334138793\n",
      "epochs = 7700, loss = 0.131, acc = 0.943, lr = 0.018570274561509396\n",
      "epochs = 7800, loss = 0.131, acc = 0.947, lr = 0.018553047801927663\n",
      "epochs = 7900, loss = 0.129, acc = 0.953, lr = 0.018535852973614212\n",
      "epochs = 8000, loss = 0.129, acc = 0.950, lr = 0.01851868998787026\n",
      "epochs = 8100, loss = 0.128, acc = 0.947, lr = 0.018501558756325222\n",
      "epochs = 8200, loss = 0.129, acc = 0.950, lr = 0.01848445919093522\n",
      "epochs = 8300, loss = 0.128, acc = 0.947, lr = 0.018467391203981567\n",
      "epochs = 8400, loss = 0.126, acc = 0.950, lr = 0.01845035470806926\n",
      "epochs = 8500, loss = 0.129, acc = 0.950, lr = 0.018433349616125496\n",
      "epochs = 8600, loss = 0.132, acc = 0.953, lr = 0.018416375841398172\n",
      "epochs = 8700, loss = 0.126, acc = 0.953, lr = 0.018399433297454436\n",
      "epochs = 8800, loss = 0.125, acc = 0.947, lr = 0.01838252189817921\n",
      "epochs = 8900, loss = 0.126, acc = 0.953, lr = 0.018365641557773718\n",
      "epochs = 9000, loss = 0.124, acc = 0.950, lr = 0.018348792190754044\n",
      "epochs = 9100, loss = 0.124, acc = 0.953, lr = 0.0183319737119497\n",
      "epochs = 9200, loss = 0.124, acc = 0.953, lr = 0.018315186036502167\n",
      "epochs = 9300, loss = 0.122, acc = 0.950, lr = 0.018298429079863496\n",
      "epochs = 9400, loss = 0.130, acc = 0.953, lr = 0.018281702757794862\n",
      "epochs = 9500, loss = 0.126, acc = 0.957, lr = 0.018265006986365174\n",
      "epochs = 9600, loss = 0.120, acc = 0.950, lr = 0.018248341681949654\n",
      "epochs = 9700, loss = 0.120, acc = 0.953, lr = 0.018231706761228456\n",
      "epochs = 9800, loss = 0.119, acc = 0.947, lr = 0.01821510214118526\n",
      "epochs = 9900, loss = 0.214, acc = 0.917, lr = 0.018198527739105907\n",
      "epochs = 10000, loss = 0.119, acc = 0.950, lr = 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "X,y = spiral_data(100,3)\n",
    "\n",
    "dense1 = Dense(2,64)\n",
    "activation1 = ReluActivation()\n",
    "\n",
    "dense2 = Dense(64,3)\n",
    "softmax_loss = Softmax_CrossentLoss()\n",
    "\n",
    "optimizer = Optimizer_ADAM(learning_rate=0.02,decay=1e-5)\n",
    "\n",
    "for epochs in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = softmax_loss.forward(dense2.output,y)\n",
    "\n",
    "    prediction = np.argmax(softmax_loss.softmax_output,axis=1)\n",
    "    if(len(y.shape) == 2):\n",
    "        y = np.argmax(y,axis=1)\n",
    "\n",
    "    accuracy = np.mean(prediction == y)\n",
    "    if epochs % 100 == 0:\n",
    "        print(f'epochs = {epochs}, loss = {loss:.3f}, acc = {accuracy:.3f}, lr = {optimizer.current_learning_rate}')\n",
    "\n",
    "\n",
    "    softmax_loss.backward(softmax_loss.softmax_output,y)\n",
    "    dense2.backward(softmax_loss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
